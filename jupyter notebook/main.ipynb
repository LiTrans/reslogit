{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import optimizers\n",
    "import yaml\n",
    "\n",
    "from theano import shared, function\n",
    "from scipy import stats\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from models import Logit, ResNet, MLP\n",
    "from core import shared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FLOATX = theano.config.floatX\n",
    "\n",
    "# read data file from .csv\n",
    "raw_data = pd.read_csv('data-20190702_2.csv')\n",
    "\n",
    "# read configuration file\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "# keep track of time\n",
    "config['timestamp'] = dt.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# defines the inputs and output\n",
    "x_data = raw_data.iloc[:, 1:-1]\n",
    "y_data = raw_data['mode']-1  # -1 for indexing at 0\n",
    "\n",
    "# defines the list of explanatory variable names\n",
    "config['variables'] = list(x_data.columns)\n",
    "\n",
    "# number of observations\n",
    "config['n_obs'] = raw_data.shape[0] \n",
    "\n",
    "# number of variables/choices\n",
    "config['n_vars'] = x_data.shape[1]\n",
    "config['n_choices'] = len(config['choices'])\n",
    "\n",
    "# slicing index for train/valid split\n",
    "slice = np.floor(0.7*config['n_obs']).astype(int)\n",
    "config['slice'] = slice\n",
    "\n",
    "# slices x and y datasets into train/valid\n",
    "train_x_data, valid_x_data = x_data.iloc[:slice], x_data.iloc[slice:]\n",
    "train_y_data, valid_y_data = y_data.iloc[:slice], y_data.iloc[slice:]\n",
    "\n",
    "# load train/valid datasets into shared module\n",
    "train_x_shared, train_y_shared = shared_dataset(train_x_data, train_y_data)\n",
    "valid_x_shared, valid_y_shared = shared_dataset(valid_x_data, valid_y_data)\n",
    "\n",
    "# number of train/valid batches\n",
    "n_train_batches = train_y_data.shape[0] // config['batch_size']\n",
    "n_valid_batches = valid_y_data.shape[0] // config['batch_size']\n",
    "config['n_train_batches'] = n_train_batches\n",
    "config['n_valid_batches'] = n_valid_batches\n",
    "\n",
    "# Theano tensor variables\n",
    "idx = T.lscalar()  # index to [mini]batch\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet\n"
     ]
    }
   ],
   "source": [
    "if config['model_type'] == 'ResNet':\n",
    "    # create ResNet model\n",
    "    model = ResNet(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MLP':\n",
    "    # create MLP model\n",
    "    model = MLP(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MNL':\n",
    "    # create MNL model\n",
    "    config['n_layers'] = 0\n",
    "    model = Logit(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "print(config['model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "model.train_model = function(\n",
    "    inputs=[idx],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.validate_model = function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "model.predict_model = function(\n",
    "    inputs=[],\n",
    "    outputs=model.errors(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "model.hessians = function(\n",
    "    inputs=[idx],\n",
    "    outputs=model.get_gessians(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 200/2112, validation likelihood 21961.20\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 400/2112, validation likelihood 21195.59\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 600/2112, validation likelihood 20154.71\n",
      "validation error  41.74%\n",
      "epoch 1, minibatch 800/2112, validation likelihood 19164.02\n",
      "validation error  38.23%\n",
      "epoch 1, minibatch 1000/2112, validation likelihood 18705.22\n",
      "validation error  36.29%\n",
      "epoch 1, minibatch 1200/2112, validation likelihood 18416.65\n",
      "validation error  34.25%\n",
      "epoch 1, minibatch 1400/2112, validation likelihood 18332.56\n",
      "validation error  31.95%\n",
      "epoch 1, minibatch 1600/2112, validation likelihood 18260.72\n",
      "validation error  34.86%\n",
      "epoch 1, minibatch 1800/2112, validation likelihood 17497.88\n",
      "validation error  33.32%\n",
      "epoch 1, minibatch 2000/2112, validation likelihood 17219.57\n",
      "validation error  31.65%\n",
      "epoch 2, minibatch 88/2112, validation likelihood 16991.09\n",
      "validation error  30.79%\n",
      "epoch 2, minibatch 288/2112, validation likelihood 16919.77\n",
      "validation error  30.48%\n",
      "epoch 2, minibatch 488/2112, validation likelihood 16566.14\n",
      "validation error  30.30%\n",
      "epoch 2, minibatch 688/2112, validation likelihood 16463.18\n",
      "validation error  29.64%\n",
      "epoch 2, minibatch 1088/2112, validation likelihood 16370.59\n",
      "validation error  29.33%\n",
      "epoch 2, minibatch 1288/2112, validation likelihood 16270.10\n",
      "validation error  29.09%\n",
      "epoch 2, minibatch 1888/2112, validation likelihood 15981.47\n",
      "validation error  28.99%\n",
      "epoch 2, minibatch 2088/2112, validation likelihood 15810.37\n",
      "validation error  28.49%\n",
      "epoch 3, minibatch 776/2112, validation likelihood 15722.56\n",
      "validation error  28.16%\n",
      "epoch 3, minibatch 1376/2112, validation likelihood 15689.36\n",
      "validation error  28.06%\n",
      "epoch 3, minibatch 1776/2112, validation likelihood 15619.27\n",
      "validation error  28.13%\n",
      "epoch 3, minibatch 1976/2112, validation likelihood 15535.56\n",
      "validation error  28.13%\n",
      "epoch 4, minibatch 464/2112, validation likelihood 15412.90\n",
      "validation error  27.76%\n",
      "epoch 4, minibatch 1864/2112, validation likelihood 15275.29\n",
      "validation error  27.51%\n",
      "epoch 4, minibatch 2064/2112, validation likelihood 15270.82\n",
      "validation error  27.52%\n",
      "epoch 5, minibatch 1352/2112, validation likelihood 15231.83\n",
      "validation error  27.61%\n",
      "epoch 5, minibatch 1952/2112, validation likelihood 15185.81\n",
      "validation error  27.17%\n",
      "epoch 6, minibatch 40/2112, validation likelihood 15115.78\n",
      "validation error  26.63%\n",
      "epoch 6, minibatch 1240/2112, validation likelihood 15089.37\n",
      "validation error  27.17%\n",
      "epoch 6, minibatch 1840/2112, validation likelihood 14986.22\n",
      "validation error  26.70%\n",
      "epoch 7, minibatch 1728/2112, validation likelihood 14940.97\n",
      "validation error  26.90%\n",
      "epoch 7, minibatch 1928/2112, validation likelihood 14882.78\n",
      "validation error  26.82%\n",
      "epoch 8, minibatch 1216/2112, validation likelihood 14856.50\n",
      "validation error  26.75%\n",
      "epoch 8, minibatch 1616/2112, validation likelihood 14814.16\n",
      "validation error  26.86%\n",
      "epoch 8, minibatch 1816/2112, validation likelihood 14779.82\n",
      "validation error  26.69%\n",
      "epoch 9, minibatch 504/2112, validation likelihood 14726.03\n",
      "validation error  26.53%\n",
      "epoch 9, minibatch 2104/2112, validation likelihood 14697.64\n",
      "validation error  26.68%\n",
      "epoch 10, minibatch 1792/2112, validation likelihood 14616.03\n",
      "validation error  26.57%\n",
      "epoch 11, minibatch 480/2112, validation likelihood 14598.50\n",
      "validation error  26.58%\n",
      "epoch 11, minibatch 1880/2112, validation likelihood 14568.03\n",
      "validation error  26.45%\n",
      "epoch 11, minibatch 2080/2112, validation likelihood 14565.34\n",
      "validation error  26.64%\n",
      "epoch 12, minibatch 368/2112, validation likelihood 14538.13\n",
      "validation error  26.40%\n",
      "epoch 13, minibatch 456/2112, validation likelihood 14464.18\n",
      "validation error  26.27%\n",
      "epoch 14, minibatch 144/2112, validation likelihood 14454.17\n",
      "validation error  26.17%\n",
      "epoch 14, minibatch 344/2112, validation likelihood 14450.10\n",
      "validation error  26.11%\n",
      "epoch 14, minibatch 1744/2112, validation likelihood 14442.89\n",
      "validation error  26.29%\n",
      "epoch 15, minibatch 32/2112, validation likelihood 14370.18\n",
      "validation error  26.06%\n",
      "epoch 15, minibatch 2032/2112, validation likelihood 14356.69\n",
      "validation error  25.88%\n",
      "epoch 16, minibatch 120/2112, validation likelihood 14350.01\n",
      "validation error  25.92%\n",
      "epoch 16, minibatch 1920/2112, validation likelihood 14344.73\n",
      "validation error  26.49%\n",
      "epoch 17, minibatch 8/2112, validation likelihood 14330.48\n",
      "validation error  26.09%\n",
      "epoch 17, minibatch 1008/2112, validation likelihood 14295.70\n",
      "validation error  26.34%\n",
      "epoch 18, minibatch 496/2112, validation likelihood 14186.93\n",
      "validation error  26.07%\n",
      "epoch 20, minibatch 472/2112, validation likelihood 14159.78\n",
      "validation error  26.30%\n",
      "epoch 22, minibatch 848/2112, validation likelihood 14129.84\n",
      "validation error  26.10%\n",
      "epoch 24, minibatch 24/2112, validation likelihood 14109.42\n",
      "validation error  25.85%\n",
      "epoch 24, minibatch 2024/2112, validation likelihood 14086.62\n",
      "validation error  25.46%\n",
      "epoch 26, minibatch 200/2112, validation likelihood 14084.41\n",
      "validation error  25.62%\n",
      "epoch 26, minibatch 1400/2112, validation likelihood 14067.86\n",
      "validation error  25.96%\n",
      "epoch 26, minibatch 1800/2112, validation likelihood 14017.70\n",
      "validation error  26.38%\n",
      "epoch 27, minibatch 488/2112, validation likelihood 13998.80\n",
      "validation error  25.81%\n",
      "epoch 27, minibatch 688/2112, validation likelihood 13903.40\n",
      "validation error  25.32%\n",
      "epoch 27, minibatch 2088/2112, validation likelihood 13869.62\n",
      "validation error  25.18%\n",
      "epoch 28, minibatch 1776/2112, validation likelihood 13862.85\n",
      "validation error  25.25%\n",
      "epoch 29, minibatch 464/2112, validation likelihood 13858.10\n",
      "validation error  25.49%\n",
      "epoch 29, minibatch 664/2112, validation likelihood 13828.04\n",
      "validation error  24.65%\n",
      "epoch 29, minibatch 2064/2112, validation likelihood 13804.16\n",
      "validation error  24.50%\n",
      "epoch 30, minibatch 152/2112, validation likelihood 13693.18\n",
      "validation error  24.41%\n",
      "epoch 33, minibatch 16/2112, validation likelihood 13567.91\n",
      "validation error  24.43%\n",
      "epoch 33, minibatch 1016/2112, validation likelihood 13559.40\n",
      "validation error  24.47%\n",
      "epoch 33, minibatch 1816/2112, validation likelihood 13519.77\n",
      "validation error  24.47%\n",
      "epoch 33, minibatch 2016/2112, validation likelihood 13513.17\n",
      "validation error  23.90%\n",
      "epoch 34, minibatch 504/2112, validation likelihood 13441.07\n",
      "validation error  23.89%\n",
      "epoch 34, minibatch 1304/2112, validation likelihood 13435.46\n",
      "validation error  24.08%\n",
      "epoch 35, minibatch 1392/2112, validation likelihood 13413.27\n",
      "validation error  24.23%\n",
      "epoch 36, minibatch 1880/2112, validation likelihood 13375.61\n",
      "validation error  24.00%\n",
      "epoch 36, minibatch 2080/2112, validation likelihood 13358.64\n",
      "validation error  23.73%\n",
      "epoch 37, minibatch 368/2112, validation likelihood 13298.69\n",
      "validation error  23.22%\n",
      "epoch 38, minibatch 1856/2112, validation likelihood 13298.33\n",
      "validation error  23.72%\n",
      "epoch 40, minibatch 32/2112, validation likelihood 13277.45\n",
      "validation error  23.52%\n",
      "epoch 43, minibatch 496/2112, validation likelihood 13149.53\n",
      "validation error  23.13%\n",
      "epoch 197, minibatch 2048/2112, validation likelihood 13121.92\n",
      "validation error  23.27%\n",
      "processing model statistics...\n",
      "Optimization complete with best validation likelihood of 13121.92, and validation error of 23.27%\n",
      "The code run for 201 epochs, with 0.65 epochs/sec\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "valid_freq = min(200, n_train_batches)\n",
    "best_validation_ll = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "alid_xalid_x\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "step = 0\n",
    "\n",
    "filename = '{}{}_bestmodel.pkl'.format(config['model_type'], config['n_layers'])\n",
    "training_frame = pd.DataFrame(\n",
    "    columns=['epoch', 'minibatch', 'batches', 'train_ll', 'valid_ll', 'valid_err']\n",
    ")\n",
    "\n",
    "while (epoch < config['n_epochs']) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    training_ll = 0\n",
    "    for i in range(n_train_batches):\n",
    "        # accumulating average score\n",
    "        minibatch_ll = model.train_model(i)\n",
    "        training_ll = (training_ll * i + minibatch_ll)/(i + 1)\n",
    "        \n",
    "        iteration = (epoch - 1) * n_train_batches + i\n",
    "\n",
    "        if (iteration + 1) % valid_freq == 0:\n",
    "            validation_ll = np.sum(model.validate_model())\n",
    "            #################################\n",
    "            # track and save training stats #\n",
    "            #################################\n",
    "            training_step = {\n",
    "                'epoch': epoch, \n",
    "                'minibatch': i + 1, \n",
    "                'batches': n_train_batches, \n",
    "                'train_ll': training_ll * n_train_batches, \n",
    "                'valid_ll': validation_ll, \n",
    "                'valid_err': None,\n",
    "            }\n",
    "            training_frame.loc[step] = training_step\n",
    "            #################################\n",
    "            \n",
    "            if validation_ll < best_validation_ll:\n",
    "                print(('epoch {:d}, minibatch {:d}/{:d}, '\n",
    "                       'validation likelihood {:.2f}').format(\n",
    "                        epoch, i + 1, n_train_batches, validation_ll))\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if validation_ll < best_validation_ll * config['improvement_threshold']:\n",
    "                    config['patience'] = max(config['patience'], iteration * config['patience_increase'])\n",
    "\n",
    "                # keep track of best validation score\n",
    "                best_validation_ll = validation_ll\n",
    "\n",
    "                # check prediction accuracy\n",
    "                error = np.mean(model.predict_model())\n",
    "\n",
    "                # set valid error in training frame\n",
    "                training_frame.loc[step, 'valid_err'] = error\n",
    "                print('validation error  {:.2%}'.format(error))\n",
    "\n",
    "                # save the best model\n",
    "                with open(filename, 'wb') as f:\n",
    "                    pickle.dump([model, config], f)\n",
    "\n",
    "            step = step + 1\n",
    "\n",
    "        if epoch > 200:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "#############################\n",
    "# parameter extraction step #\n",
    "#############################\n",
    "print('processing model statistics...')\n",
    "with open(filename, 'rb') as f:\n",
    "    model, config = pickle.load(f)\n",
    "\n",
    "# format model beta params and ASCs\n",
    "model_stat = {}\n",
    "if config['model_type'] in ['MNL', 'ResNet']:\n",
    "    beta = model.beta.eval().round(3)\n",
    "    beta_df = pd.DataFrame(beta, index=config['variables'], columns=config['choices'])\n",
    "    model_stat['beta_params'] = beta_df\n",
    "\n",
    "    asc = model.params[1].eval().round(3)\n",
    "    asc_df = pd.DataFrame(asc.reshape((1,-1)), index=['ASC'], columns=config['choices'])\n",
    "    model_stat['asc_params'] = asc_df\n",
    "\n",
    "    # compute std. err and t-stat\n",
    "    h = np.mean([model.hessians(i) for i in range(n_train_batches)], axis=0)\n",
    "    model_stat['beta_stderr'] = pd.DataFrame(\n",
    "        np.sqrt(1/np.diag(h[0]).reshape((config['n_vars'], config['n_choices'])))/(batch_size-1), \n",
    "        index=config['variables'], \n",
    "        columns=config['choices']\n",
    "    )\n",
    "    model_stat['beta_t_stat'] = pd.DataFrame(\n",
    "        beta / model_stat['beta_stderr'], index=config['variables'], columns=config['choices'])\n",
    "    \n",
    "# format ResNet residual matrix\n",
    "if config['model_type'] == 'ResNet':\n",
    "    model_stat['residual_matrix'] = []\n",
    "    for l in range(config['n_layers']):\n",
    "        # create a pandas correlation matrix table\n",
    "        mat = model.resnet_layers[l].params[0].eval().round(2)\n",
    "        df = pd.DataFrame(\n",
    "            data=mat, index=config['choices'], columns=config['choices'])\n",
    "        model_stat['residual_matrix'].append(df)\n",
    "\n",
    "# misc: runtime and training curves\n",
    "model_stat['run_time'] = str(np.round(run_time / 60., 3))+' minutes'\n",
    "model_stat['training_frame'] = training_frame\n",
    "\n",
    "# re-save model, configuration and statistics     \n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump([model, config, model_stat], f)        \n",
    "#############################\n",
    "\n",
    "# print final verbose output\n",
    "print(('Optimization complete with best validation likelihood of {:.2f}, '\n",
    "       'and validation error of {:.2%}').format(best_validation_ll, error))\n",
    "print(('The code run for {:d} epochs, with {:.2f} epochs/sec').format(\n",
    "        epoch, 1. * epoch / run_time))\n",
    "print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['model_type'] == 'ResNet': \n",
    "#     print(model_stat['residual_matrix'])\n",
    "# MNL Optimization complete with best validation likelihood of 16145.33, and validation error of 27.99%\n",
    "# MNL-2 Optimization complete with best validation likelihood of 15583.91, and validation error of 27.43%\n",
    "# MLP-4 Optimization complete with best validation likelihood of 15894.59, and validation error of 28.09%\n",
    "# MLP-8 Optimization complete with best validation likelihood of 16736.20, and validation error of 30.17%\n",
    "# MLP-16 Optimization complete with best validation likelihood of 16667.83, and validation error of 30.50%\n",
    "\n",
    "# ResNet-2 Optimization complete with best validation likelihood of 13675.01, and validation error of 24.82%\n",
    "# ResNet-4 Optimization complete with best validation likelihood of 13583.95, and validation error of 23.85%\n",
    "# ResNet-8 Optimization complete with best validation likelihood of 12870.18, and validation error of 22.71%\n",
    "# ResNet-16 Optimization complete with best validation likelihood of 13121.92, and validation error of 23.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MNL0_bestmodel.pkl'\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "model, config, model_stat = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat['beta_params'].stack().reset_index().iloc[[0,1,2,10,11,15,18,28,36,70,72,73,77,79,80,84,87,91,94,133,134,135,140,142,144,148]][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

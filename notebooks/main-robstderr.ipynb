{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import yaml\n",
    "\n",
    "from theano import shared, function\n",
    "from scipy import stats\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from reslogit.models import Logit, ResNet, MLP\n",
    "from reslogit.core import *\n",
    "import reslogit.optimizers as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FLOATX = theano.config.floatX\n",
    "\n",
    "# read data file from .csv\n",
    "raw_data = pd.read_csv(os.getcwd() + '/data/data-20190702_2.csv')\n",
    "\n",
    "# read configuration file\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "# keep track of time\n",
    "config['timestamp'] = dt.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# defines the inputs and output\n",
    "x_data = raw_data.iloc[:, 1:-1]\n",
    "y_data = raw_data['mode']-1  # -1 for indexing at 0\n",
    "\n",
    "# defines the list of explanatory variable names\n",
    "config['variables'] = list(x_data.columns)\n",
    "\n",
    "# number of observations\n",
    "config['n_obs'] = raw_data.shape[0] \n",
    "\n",
    "# number of variables/choices\n",
    "config['n_vars'] = x_data.shape[1]\n",
    "config['n_choices'] = len(config['choices'])\n",
    "\n",
    "# slicing index for train/valid split\n",
    "slice = np.floor(0.7*config['n_obs']).astype(int)\n",
    "config['slice'] = slice\n",
    "\n",
    "# slices x and y datasets into train/valid\n",
    "train_x_data, valid_x_data = x_data.iloc[:slice], x_data.iloc[slice:]\n",
    "train_y_data, valid_y_data = y_data.iloc[:slice], y_data.iloc[slice:]\n",
    "\n",
    "# load train/valid datasets into shared module\n",
    "train_x_shared, train_y_shared = shared_dataset(train_x_data, train_y_data)\n",
    "valid_x_shared, valid_y_shared = shared_dataset(valid_x_data, valid_y_data)\n",
    "\n",
    "# number of train/valid batches\n",
    "n_train_batches = train_y_data.shape[0] // config['batch_size']\n",
    "n_valid_batches = valid_y_data.shape[0] // config['batch_size']\n",
    "config['n_train_batches'] = n_train_batches\n",
    "config['n_valid_batches'] = n_valid_batches\n",
    "\n",
    "# Theano tensor variables\n",
    "idx = T.lscalar()  # index to [mini]batch\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet\n"
     ]
    }
   ],
   "source": [
    "if config['model_type'] == 'ResNet':\n",
    "    # create ResNet model\n",
    "    model = ResNet(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MLP':\n",
    "    # create MLP model\n",
    "    model = MLP(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MNL':\n",
    "    # create MNL model\n",
    "    config['n_layers'] = 0\n",
    "    model = Logit(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "print(config['model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "model.train_model = function(\n",
    "    inputs=[idx],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.validate_model = function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "model.predict_model = function(\n",
    "    inputs=[],\n",
    "    outputs=model.errors(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "model.train_loss_model = function(\n",
    "    inputs=[],\n",
    "    outputs=model.errors(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared,\n",
    "        y: train_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.hessians = function(\n",
    "    inputs=[idx],\n",
    "    outputs=model.get_gessians(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.derivatives = function(\n",
    "    inputs=[idx],\n",
    "    outputs=model.get_derivatives(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 200/1320, validation likelihood 21941.07\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 400/1320, validation likelihood 20857.05\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 600/1320, validation likelihood 19934.73\n",
      "validation error  40.51%\n",
      "epoch 1, minibatch 800/1320, validation likelihood 19060.56\n",
      "validation error  38.00%\n",
      "epoch 1, minibatch 1000/1320, validation likelihood 18991.36\n",
      "validation error  38.16%\n",
      "epoch 1, minibatch 1200/1320, validation likelihood 17684.20\n",
      "validation error  33.74%\n",
      "epoch 2, minibatch 80/1320, validation likelihood 17282.93\n",
      "validation error  31.20%\n",
      "epoch 2, minibatch 280/1320, validation likelihood 17078.30\n",
      "validation error  32.15%\n",
      "epoch 2, minibatch 480/1320, validation likelihood 16836.37\n",
      "validation error  31.21%\n",
      "epoch 2, minibatch 680/1320, validation likelihood 16662.06\n",
      "validation error  30.53%\n",
      "epoch 2, minibatch 880/1320, validation likelihood 16604.41\n",
      "validation error  29.35%\n",
      "epoch 2, minibatch 1080/1320, validation likelihood 16430.09\n",
      "validation error  28.95%\n",
      "epoch 2, minibatch 1280/1320, validation likelihood 16179.64\n",
      "validation error  29.01%\n",
      "epoch 3, minibatch 160/1320, validation likelihood 16057.07\n",
      "validation error  29.27%\n",
      "epoch 3, minibatch 760/1320, validation likelihood 15839.96\n",
      "validation error  28.44%\n",
      "epoch 3, minibatch 1160/1320, validation likelihood 15649.26\n",
      "validation error  28.34%\n",
      "epoch 4, minibatch 240/1320, validation likelihood 15582.79\n",
      "validation error  28.16%\n",
      "epoch 4, minibatch 840/1320, validation likelihood 15550.82\n",
      "validation error  28.19%\n",
      "epoch 4, minibatch 1240/1320, validation likelihood 15540.99\n",
      "validation error  28.16%\n",
      "epoch 5, minibatch 120/1320, validation likelihood 15373.60\n",
      "validation error  28.02%\n",
      "epoch 5, minibatch 320/1320, validation likelihood 15365.70\n",
      "validation error  27.93%\n",
      "epoch 5, minibatch 1120/1320, validation likelihood 15187.54\n",
      "validation error  27.47%\n",
      "epoch 5, minibatch 1320/1320, validation likelihood 15163.01\n",
      "validation error  27.31%\n",
      "epoch 6, minibatch 800/1320, validation likelihood 15128.07\n",
      "validation error  27.37%\n",
      "epoch 6, minibatch 1200/1320, validation likelihood 15038.54\n",
      "validation error  27.17%\n",
      "epoch 7, minibatch 1080/1320, validation likelihood 15006.81\n",
      "validation error  27.00%\n",
      "epoch 8, minibatch 160/1320, validation likelihood 14998.03\n",
      "validation error  27.03%\n",
      "epoch 8, minibatch 760/1320, validation likelihood 14883.64\n",
      "validation error  26.69%\n",
      "epoch 8, minibatch 1160/1320, validation likelihood 14844.51\n",
      "validation error  26.65%\n",
      "epoch 10, minibatch 120/1320, validation likelihood 14819.40\n",
      "validation error  26.78%\n",
      "epoch 10, minibatch 1120/1320, validation likelihood 14701.70\n",
      "validation error  26.63%\n",
      "epoch 10, minibatch 1320/1320, validation likelihood 14700.63\n",
      "validation error  26.60%\n",
      "epoch 11, minibatch 1200/1320, validation likelihood 14680.99\n",
      "validation error  26.66%\n",
      "epoch 12, minibatch 1080/1320, validation likelihood 14675.84\n",
      "validation error  26.73%\n",
      "epoch 13, minibatch 760/1320, validation likelihood 14616.18\n",
      "validation error  26.46%\n",
      "epoch 13, minibatch 1160/1320, validation likelihood 14571.12\n",
      "validation error  26.56%\n",
      "epoch 15, minibatch 320/1320, validation likelihood 14547.77\n",
      "validation error  26.50%\n",
      "epoch 15, minibatch 1120/1320, validation likelihood 14431.64\n",
      "validation error  26.53%\n",
      "epoch 15, minibatch 1320/1320, validation likelihood 14428.67\n",
      "validation error  26.53%\n",
      "epoch 16, minibatch 1200/1320, validation likelihood 14427.24\n",
      "validation error  26.57%\n",
      "epoch 17, minibatch 480/1320, validation likelihood 14412.02\n",
      "validation error  26.48%\n",
      "epoch 18, minibatch 760/1320, validation likelihood 14399.14\n",
      "validation error  26.40%\n",
      "epoch 19, minibatch 240/1320, validation likelihood 14328.15\n",
      "validation error  26.41%\n",
      "epoch 20, minibatch 320/1320, validation likelihood 14319.53\n",
      "validation error  26.42%\n",
      "epoch 20, minibatch 1120/1320, validation likelihood 14227.73\n",
      "validation error  26.21%\n",
      "epoch 24, minibatch 240/1320, validation likelihood 14150.53\n",
      "validation error  26.22%\n",
      "epoch 25, minibatch 320/1320, validation likelihood 14132.29\n",
      "validation error  26.20%\n",
      "epoch 25, minibatch 1120/1320, validation likelihood 14054.91\n",
      "validation error  26.13%\n",
      "epoch 29, minibatch 240/1320, validation likelihood 13974.80\n",
      "validation error  26.10%\n",
      "epoch 30, minibatch 320/1320, validation likelihood 13939.41\n",
      "validation error  25.62%\n",
      "epoch 30, minibatch 1120/1320, validation likelihood 13880.42\n",
      "validation error  25.89%\n",
      "epoch 32, minibatch 480/1320, validation likelihood 13866.18\n",
      "validation error  25.50%\n",
      "epoch 33, minibatch 1160/1320, validation likelihood 13806.06\n",
      "validation error  25.18%\n",
      "epoch 34, minibatch 240/1320, validation likelihood 13717.75\n",
      "validation error  25.45%\n",
      "epoch 35, minibatch 320/1320, validation likelihood 13699.30\n",
      "validation error  24.77%\n",
      "epoch 35, minibatch 1120/1320, validation likelihood 13634.21\n",
      "validation error  25.32%\n",
      "epoch 38, minibatch 1160/1320, validation likelihood 13514.39\n",
      "validation error  24.22%\n",
      "epoch 39, minibatch 240/1320, validation likelihood 13441.54\n",
      "validation error  24.46%\n",
      "epoch 40, minibatch 320/1320, validation likelihood 13406.14\n",
      "validation error  24.11%\n",
      "epoch 42, minibatch 80/1320, validation likelihood 13386.85\n",
      "validation error  23.87%\n",
      "epoch 43, minibatch 1160/1320, validation likelihood 13293.32\n",
      "validation error  23.80%\n",
      "epoch 47, minibatch 80/1320, validation likelihood 13267.11\n",
      "validation error  23.71%\n",
      "epoch 48, minibatch 1160/1320, validation likelihood 13205.30\n",
      "validation error  23.57%\n",
      "epoch 53, minibatch 1160/1320, validation likelihood 13183.77\n",
      "validation error  23.69%\n",
      "epoch 55, minibatch 1120/1320, validation likelihood 13169.80\n",
      "validation error  23.48%\n",
      "epoch 57, minibatch 80/1320, validation likelihood 13166.00\n",
      "validation error  23.31%\n",
      "epoch 58, minibatch 1160/1320, validation likelihood 13161.36\n",
      "validation error  23.61%\n",
      "epoch 62, minibatch 80/1320, validation likelihood 13146.94\n",
      "validation error  23.31%\n",
      "epoch 63, minibatch 1160/1320, validation likelihood 13135.47\n",
      "validation error  23.56%\n",
      "epoch 68, minibatch 1160/1320, validation likelihood 13099.89\n",
      "validation error  23.41%\n",
      "epoch 73, minibatch 1160/1320, validation likelihood 13066.34\n",
      "validation error  23.25%\n",
      "epoch 78, minibatch 1160/1320, validation likelihood 13041.48\n",
      "validation error  23.40%\n",
      "epoch 83, minibatch 1160/1320, validation likelihood 12987.74\n",
      "validation error  23.50%\n",
      "epoch 88, minibatch 1160/1320, validation likelihood 12961.94\n",
      "validation error  23.51%\n",
      "epoch 93, minibatch 1160/1320, validation likelihood 12940.85\n",
      "validation error  23.42%\n",
      "epoch 98, minibatch 1160/1320, validation likelihood 12938.32\n",
      "validation error  23.30%\n"
     ]
    }
   ],
   "source": [
    "valid_freq = min(200, n_train_batches)\n",
    "best_validation_ll = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "step = 0\n",
    "\n",
    "filename = '{}{}_bestmodel.pkl'.format(config['model_type'], config['n_layers'])\n",
    "training_frame = pd.DataFrame(\n",
    "    columns=['epoch', 'minibatch', 'batches', 'train_ll', 'valid_ll', 'valid_err', 'train_err']\n",
    ")\n",
    "\n",
    "while (epoch < config['n_epochs']) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    training_ll = 0\n",
    "    for i in range(n_train_batches):\n",
    "        # accumulating average score\n",
    "        minibatch_ll = model.train_model(i)\n",
    "        training_ll = (training_ll * i + minibatch_ll)/(i + 1)\n",
    "        \n",
    "        iteration = (epoch - 1) * n_train_batches + i\n",
    "\n",
    "        if (iteration + 1) % valid_freq == 0:\n",
    "            validation_ll = np.sum(model.validate_model())\n",
    "            #################################\n",
    "            # track and save training stats #\n",
    "            #################################\n",
    "            training_step = {\n",
    "                'epoch': epoch, \n",
    "                'minibatch': i + 1, \n",
    "                'batches': n_train_batches, \n",
    "                'train_ll': training_ll * n_train_batches, \n",
    "                'valid_ll': validation_ll, \n",
    "                'valid_err': None,\n",
    "                'train_err': None,\n",
    "            }\n",
    "            training_frame.loc[step] = training_step\n",
    "            #################################\n",
    "\n",
    "            # check prediction accuracy\n",
    "            error = np.mean(model.predict_model())\n",
    "            training_frame.loc[step, 'valid_err'] = error\n",
    "\n",
    "            \n",
    "            \n",
    "            if validation_ll < best_validation_ll:\n",
    "                print(('epoch {:d}, minibatch {:d}/{:d}, '\n",
    "                       'validation likelihood {:.2f}').format(\n",
    "                        epoch, i + 1, n_train_batches, validation_ll))\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if validation_ll < best_validation_ll * config['improvement_threshold']:\n",
    "                    config['patience'] = max(config['patience'], iteration * config['patience_increase'])\n",
    "\n",
    "                # keep track of best validation score\n",
    "                best_validation_ll = validation_ll\n",
    "\n",
    "                best_error = np.mean(model.predict_model())\n",
    "                training_frame.loc[step, 'valid_err'] = best_error\n",
    "                print('validation error  {:.2%}'.format(best_error))\n",
    "\n",
    "                # save the best model\n",
    "                with open(filename, 'wb') as f:\n",
    "                    pickle.dump([model, config], f)\n",
    "\n",
    "            step = step + 1\n",
    "\n",
    "        if epoch > 200:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "#############################\n",
    "# parameter extraction step #\n",
    "#############################\n",
    "print('processing model statistics...')\n",
    "with open(filename, 'rb') as f:\n",
    "    model, config = pickle.load(f)\n",
    "\n",
    "# format model beta params and ASCs\n",
    "model_stat = {}\n",
    "if config['model_type'] in ['MNL', 'ResNet']:\n",
    "    beta = model.beta.eval().round(3)\n",
    "    beta_df = pd.DataFrame(beta, index=config['variables'], columns=config['choices'])\n",
    "    model_stat['beta_params'] = beta_df\n",
    "\n",
    "    asc = model.params[1].eval().round(3)\n",
    "    asc_df = pd.DataFrame(asc.reshape((1,-1)), index=['ASC'], columns=config['choices'])\n",
    "    model_stat['asc_params'] = asc_df\n",
    "    \n",
    "    # compute sandwich estimator \"middle\"\n",
    "    B = np.mean([model.derivatives(i) for i in range(n_train_batches)], axis=0)\n",
    "    B = B[0]\n",
    "    model_stat['sandwich_B'] = B\n",
    "    \n",
    "    # compute sandwich estimator \"bread\"\n",
    "    neg_A = np.mean([model.hessians(i) for i in range(n_train_batches)], axis=0)\n",
    "    invneg_A = 1/np.diag(neg_A[0])\n",
    "    model_stat['sandwich_invneg_A'] = invneg_A\n",
    "    \n",
    "    # compute std. err and t-stat\n",
    "    h = np.mean([model.hessians(i) for i in range(n_train_batches)], axis=0)\n",
    "    model_stat['beta_stderr'] = pd.DataFrame(\n",
    "        np.sqrt(1/np.diag(h[0]).reshape((config['n_vars'], config['n_choices'])))/(batch_size-1), \n",
    "        index=config['variables'], \n",
    "        columns=config['choices']\n",
    "    )\n",
    "    model_stat['beta_t_stat'] = pd.DataFrame(\n",
    "        beta / model_stat['beta_stderr'], index=config['variables'], columns=config['choices'])\n",
    "    \n",
    "# format ResNet residual matrix\n",
    "if config['model_type'] == 'ResNet':\n",
    "    model_stat['residual_matrix'] = []\n",
    "    for l in range(config['n_layers']):\n",
    "        # create a pandas correlation matrix table\n",
    "        mat = model.resnet_layers[l].params[0].eval().round(2)\n",
    "        df = pd.DataFrame(\n",
    "            data=mat, index=config['choices'], columns=config['choices'])\n",
    "        model_stat['residual_matrix'].append(df)\n",
    "\n",
    "# misc: runtime and training curves\n",
    "model_stat['run_time'] = str(np.round(run_time / 60., 3))+' minutes'\n",
    "model_stat['training_frame'] = training_frame\n",
    "\n",
    "# re-save model, configuration and statistics     \n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump([model, config, model_stat], f)        \n",
    "#############################\n",
    "\n",
    "# print final verbose output\n",
    "print(('Optimization complete with best validation likelihood of {:.2f}, '\n",
    "       'and validation error of {:.2%}').format(best_validation_ll, best_error))\n",
    "print(('The code run for {:d} epochs, with {:.2f} epochs/sec').format(\n",
    "        epoch, 1. * epoch / run_time))\n",
    "print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['model_type'] == 'ResNet': \n",
    "#     print(model_stat['residual_matrix'])\n",
    "# MNL Optimization complete with best validation likelihood of 15873.71, and validation error of 27.93% $$\n",
    "# MLP-2 Optimization complete with best validation likelihood of 15643.12, and validation error of 27.56% $$\n",
    "# MLP-4 Optimization complete with best validation likelihood of 17406.30, and validation error of 29.74% $$\n",
    "# MLP-8 Optimization complete with best validation likelihood of 17370.73, and validation error of 29.70% $$\n",
    "# MLP-16 Optimization complete with best validation likelihood of 17403.81, and validation error of 29.69% $$\n",
    "\n",
    "# ResNet-2 Optimization complete with best validation likelihood of 13443.44, and validation error of 23.87% $$\n",
    "# ResNet-4 Optimization complete with best validation likelihood of 13082.63, and validation error of 23.58% $$\n",
    "# ResNet-8 Optimization complete with best validation likelihood of 12894.89, and validation error of 23.33% $$\n",
    "# ResNet-16 Optimization complete with best validation likelihood of 12938.32, and validation error of 23.30% $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'ResNet16_bestmodel.pkl'\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "model, config, model_stat = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat['beta_params'].stack().reset_index().iloc[[0,1,2,10,11,15,18,28,36,70,72,73,77,79,80,84,87,91,94,133,134,135,140,142,144,148]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat['beta_stderr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model_stat[\"sandwich_B\"]\n",
    "a = model_stat[\"sandwich_invneg_A\"]\n",
    "c = np.sqrt(a * (1/b * 1/b) * a)\n",
    "\n",
    "rob_std_err = pd.DataFrame(\n",
    "        c.reshape((config['n_vars'], config['n_choices']))/(batch_size-1), \n",
    "        index=config['variables'], \n",
    "        columns=config['choices']\n",
    "    )\n",
    "rob_std_err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

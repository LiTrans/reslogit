{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import optimizers\n",
    "import yaml\n",
    "\n",
    "from theano import shared, function\n",
    "from scipy import stats\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from models import Logit, ResNet, MLP\n",
    "from core import shared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FLOATX = theano.config.floatX\n",
    "\n",
    "# read data file from .csv\n",
    "raw_data = pd.read_csv('data-20190702_2.csv')\n",
    "\n",
    "# read configuration file\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "# keep track of time\n",
    "config['timestamp'] = dt.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# defines the inputs and output\n",
    "x_data = raw_data.iloc[:, 1:-1]\n",
    "y_data = raw_data['mode']-1  # -1 for indexing at 0\n",
    "\n",
    "# defines the list of explanatory variable names\n",
    "config['variables'] = list(x_data.columns)\n",
    "\n",
    "# number of observations\n",
    "config['n_obs'] = raw_data.shape[0] \n",
    "\n",
    "# number of variables/choices\n",
    "config['n_vars'] = x_data.shape[1]\n",
    "config['n_choices'] = len(config['choices'])\n",
    "\n",
    "# slicing index for train/valid split\n",
    "slice = np.floor(0.7*config['n_obs']).astype(int)\n",
    "config['slice'] = slice\n",
    "\n",
    "# slices x and y datasets into train/valid\n",
    "train_x_data, valid_x_data = x_data.iloc[:slice], x_data.iloc[slice:]\n",
    "train_y_data, valid_y_data = y_data.iloc[:slice], y_data.iloc[slice:]\n",
    "\n",
    "# load train/valid datasets into shared module\n",
    "train_x_shared, train_y_shared = shared_dataset(train_x_data, train_y_data)\n",
    "valid_x_shared, valid_y_shared = shared_dataset(valid_x_data, valid_y_data)\n",
    "\n",
    "# number of train/valid batches\n",
    "n_train_batches = train_y_data.shape[0] // config['batch_size']\n",
    "n_valid_batches = valid_y_data.shape[0] // config['batch_size']\n",
    "config['n_train_batches'] = n_train_batches\n",
    "config['n_valid_batches'] = n_valid_batches\n",
    "\n",
    "# Theano tensor variables\n",
    "idx = T.lscalar()  # index to [mini]batch\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet\n"
     ]
    }
   ],
   "source": [
    "if config['model_type'] == 'ResNet':\n",
    "    # create ResNet model\n",
    "    model = ResNet(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MLP':\n",
    "    # create MLP model\n",
    "    model = MLP(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices'], \n",
    "        n_layers=config['n_layers']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "elif config['model_type'] == 'MNL':\n",
    "    # create MNL model\n",
    "    config['n_layers'] = 0\n",
    "    model = Logit(\n",
    "        input=x, choice=y, \n",
    "        n_vars=config['n_vars'], \n",
    "        n_choices=config['n_choices']\n",
    "    )\n",
    "    cost = model.negative_log_likelihood(y)\n",
    "    opt = optimizers.RMSProp(model.params)\n",
    "    updates = opt.run_update(\n",
    "        cost, model.params, \n",
    "        masks=model.params_mask, learning_rate=config['learning_rate']\n",
    "    )\n",
    "print(config['model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "model.train_model = function(\n",
    "    inputs=[idx],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.validate_model = function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "model.predict_model = function(\n",
    "    inputs=[],\n",
    "    outputs=model.errors(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: valid_x_shared,\n",
    "        y: valid_y_shared,\n",
    "    },\n",
    ")\n",
    "model.train_loss_model = function(\n",
    "    inputs=[],\n",
    "    outputs=model.errors(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared,\n",
    "        y: train_y_shared,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.hessians = function(\n",
    "    inputs=[idx],\n",
    "    outputs=model.get_gessians(y),\n",
    "    allow_input_downcast=True,\n",
    "    givens={\n",
    "        x: train_x_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "        y: train_y_shared[idx * batch_size: (idx + 1) * batch_size],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 200/1320, validation likelihood 21521.95\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 400/1320, validation likelihood 20764.60\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 600/1320, validation likelihood 20102.80\n",
      "validation error  41.76%\n",
      "epoch 1, minibatch 800/1320, validation likelihood 19398.03\n",
      "validation error  38.72%\n",
      "epoch 1, minibatch 1000/1320, validation likelihood 19305.61\n",
      "validation error  40.07%\n",
      "epoch 1, minibatch 1200/1320, validation likelihood 18134.97\n",
      "validation error  35.33%\n",
      "epoch 2, minibatch 80/1320, validation likelihood 17641.94\n",
      "validation error  31.40%\n",
      "epoch 2, minibatch 280/1320, validation likelihood 17348.07\n",
      "validation error  32.30%\n",
      "epoch 2, minibatch 480/1320, validation likelihood 17044.99\n",
      "validation error  31.15%\n",
      "epoch 2, minibatch 680/1320, validation likelihood 16977.64\n",
      "validation error  31.08%\n",
      "epoch 2, minibatch 1080/1320, validation likelihood 16916.72\n",
      "validation error  29.59%\n",
      "epoch 2, minibatch 1280/1320, validation likelihood 16623.64\n",
      "validation error  29.24%\n",
      "epoch 3, minibatch 160/1320, validation likelihood 16375.73\n",
      "validation error  29.38%\n",
      "epoch 3, minibatch 360/1320, validation likelihood 16350.87\n",
      "validation error  29.22%\n",
      "epoch 3, minibatch 760/1320, validation likelihood 16136.94\n",
      "validation error  28.66%\n",
      "epoch 3, minibatch 1160/1320, validation likelihood 15992.56\n",
      "validation error  28.14%\n",
      "epoch 4, minibatch 240/1320, validation likelihood 15758.47\n",
      "validation error  28.17%\n",
      "epoch 4, minibatch 640/1320, validation likelihood 15757.19\n",
      "validation error  28.53%\n",
      "epoch 4, minibatch 1240/1320, validation likelihood 15638.71\n",
      "validation error  28.24%\n",
      "epoch 5, minibatch 120/1320, validation likelihood 15504.00\n",
      "validation error  27.43%\n",
      "epoch 5, minibatch 320/1320, validation likelihood 15500.59\n",
      "validation error  27.91%\n",
      "epoch 5, minibatch 520/1320, validation likelihood 15494.60\n",
      "validation error  27.98%\n",
      "epoch 5, minibatch 1120/1320, validation likelihood 15399.21\n",
      "validation error  27.45%\n",
      "epoch 5, minibatch 1320/1320, validation likelihood 15290.60\n",
      "validation error  27.50%\n",
      "epoch 6, minibatch 1200/1320, validation likelihood 15166.98\n",
      "validation error  27.22%\n",
      "epoch 7, minibatch 480/1320, validation likelihood 15062.70\n",
      "validation error  27.22%\n",
      "epoch 8, minibatch 160/1320, validation likelihood 14956.49\n",
      "validation error  26.86%\n",
      "epoch 8, minibatch 1160/1320, validation likelihood 14868.50\n",
      "validation error  27.11%\n",
      "epoch 9, minibatch 240/1320, validation likelihood 14803.58\n",
      "validation error  27.18%\n",
      "epoch 10, minibatch 120/1320, validation likelihood 14708.61\n",
      "validation error  26.65%\n",
      "epoch 10, minibatch 1120/1320, validation likelihood 14595.78\n",
      "validation error  26.96%\n",
      "epoch 10, minibatch 1320/1320, validation likelihood 14550.51\n",
      "validation error  26.95%\n",
      "epoch 11, minibatch 800/1320, validation likelihood 14515.44\n",
      "validation error  26.68%\n",
      "epoch 11, minibatch 1200/1320, validation likelihood 14478.90\n",
      "validation error  26.93%\n",
      "epoch 12, minibatch 480/1320, validation likelihood 14445.44\n",
      "validation error  26.85%\n",
      "epoch 13, minibatch 160/1320, validation likelihood 14421.50\n",
      "validation error  26.73%\n",
      "epoch 13, minibatch 760/1320, validation likelihood 14371.49\n",
      "validation error  26.89%\n",
      "epoch 15, minibatch 120/1320, validation likelihood 14344.06\n",
      "validation error  26.45%\n",
      "epoch 15, minibatch 320/1320, validation likelihood 14333.55\n",
      "validation error  27.03%\n",
      "epoch 15, minibatch 1120/1320, validation likelihood 14261.25\n",
      "validation error  26.81%\n",
      "epoch 15, minibatch 1320/1320, validation likelihood 14244.30\n",
      "validation error  26.79%\n",
      "epoch 16, minibatch 800/1320, validation likelihood 14202.64\n",
      "validation error  26.49%\n",
      "epoch 16, minibatch 1200/1320, validation likelihood 14191.06\n",
      "validation error  26.69%\n",
      "epoch 17, minibatch 480/1320, validation likelihood 14167.85\n",
      "validation error  26.50%\n",
      "epoch 18, minibatch 760/1320, validation likelihood 14130.24\n",
      "validation error  26.56%\n",
      "epoch 20, minibatch 120/1320, validation likelihood 14100.72\n",
      "validation error  25.98%\n",
      "epoch 20, minibatch 320/1320, validation likelihood 14081.23\n",
      "validation error  26.49%\n",
      "epoch 20, minibatch 1120/1320, validation likelihood 14021.37\n",
      "validation error  26.39%\n",
      "epoch 20, minibatch 1320/1320, validation likelihood 14010.42\n",
      "validation error  26.33%\n",
      "epoch 21, minibatch 800/1320, validation likelihood 13965.35\n",
      "validation error  26.01%\n",
      "epoch 22, minibatch 480/1320, validation likelihood 13927.69\n",
      "validation error  26.12%\n",
      "epoch 23, minibatch 760/1320, validation likelihood 13908.57\n",
      "validation error  26.29%\n",
      "epoch 25, minibatch 120/1320, validation likelihood 13908.40\n",
      "validation error  24.97%\n",
      "epoch 25, minibatch 320/1320, validation likelihood 13822.61\n",
      "validation error  25.58%\n",
      "epoch 25, minibatch 1120/1320, validation likelihood 13765.22\n",
      "validation error  25.57%\n",
      "epoch 25, minibatch 1320/1320, validation likelihood 13754.41\n",
      "validation error  25.76%\n",
      "epoch 26, minibatch 800/1320, validation likelihood 13700.82\n",
      "validation error  24.87%\n",
      "epoch 27, minibatch 480/1320, validation likelihood 13613.17\n",
      "validation error  24.93%\n",
      "epoch 29, minibatch 1040/1320, validation likelihood 13603.22\n",
      "validation error  24.62%\n",
      "epoch 30, minibatch 320/1320, validation likelihood 13568.15\n",
      "validation error  24.05%\n",
      "epoch 30, minibatch 1120/1320, validation likelihood 13517.39\n",
      "validation error  24.20%\n",
      "epoch 30, minibatch 1320/1320, validation likelihood 13515.75\n",
      "validation error  25.04%\n",
      "epoch 31, minibatch 800/1320, validation likelihood 13484.38\n",
      "validation error  24.05%\n",
      "epoch 32, minibatch 480/1320, validation likelihood 13401.82\n",
      "validation error  23.68%\n",
      "epoch 36, minibatch 800/1320, validation likelihood 13395.27\n",
      "validation error  23.94%\n",
      "epoch 37, minibatch 480/1320, validation likelihood 13323.73\n",
      "validation error  23.29%\n",
      "epoch 42, minibatch 480/1320, validation likelihood 13289.24\n",
      "validation error  23.19%\n",
      "epoch 45, minibatch 920/1320, validation likelihood 13288.80\n",
      "validation error  23.45%\n",
      "epoch 47, minibatch 480/1320, validation likelihood 13247.27\n",
      "validation error  23.22%\n",
      "epoch 48, minibatch 760/1320, validation likelihood 13229.90\n",
      "validation error  23.63%\n",
      "epoch 50, minibatch 920/1320, validation likelihood 13225.32\n",
      "validation error  23.35%\n",
      "epoch 51, minibatch 800/1320, validation likelihood 13212.26\n",
      "validation error  23.50%\n",
      "epoch 52, minibatch 480/1320, validation likelihood 13168.20\n",
      "validation error  23.16%\n",
      "epoch 52, minibatch 1080/1320, validation likelihood 13165.14\n",
      "validation error  23.26%\n",
      "epoch 53, minibatch 1160/1320, validation likelihood 13145.68\n",
      "validation error  23.45%\n",
      "epoch 55, minibatch 320/1320, validation likelihood 13133.02\n",
      "validation error  23.10%\n",
      "epoch 57, minibatch 480/1320, validation likelihood 13131.28\n",
      "validation error  23.18%\n",
      "epoch 58, minibatch 1160/1320, validation likelihood 13111.04\n",
      "validation error  23.42%\n",
      "epoch 59, minibatch 240/1320, validation likelihood 13107.94\n",
      "validation error  23.48%\n",
      "epoch 60, minibatch 320/1320, validation likelihood 13106.08\n",
      "validation error  23.04%\n",
      "epoch 64, minibatch 240/1320, validation likelihood 13097.28\n",
      "validation error  23.45%\n",
      "epoch 69, minibatch 240/1320, validation likelihood 13082.63\n",
      "validation error  23.58%\n",
      "processing model statistics...\n",
      "Optimization complete with best validation likelihood of 13082.63, and validation error of 25.01%\n",
      "The code run for 201 epochs, with 0.88 epochs/sec\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "valid_freq = min(200, n_train_batches)\n",
    "best_validation_ll = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "step = 0\n",
    "\n",
    "filename = '{}{}_bestmodel.pkl'.format(config['model_type'], config['n_layers'])\n",
    "training_frame = pd.DataFrame(\n",
    "    columns=['epoch', 'minibatch', 'batches', 'train_ll', 'valid_ll', 'valid_err', 'train_err']\n",
    ")\n",
    "\n",
    "while (epoch < config['n_epochs']) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    training_ll = 0\n",
    "    for i in range(n_train_batches):\n",
    "        # accumulating average score\n",
    "        minibatch_ll = model.train_model(i)\n",
    "        training_ll = (training_ll * i + minibatch_ll)/(i + 1)\n",
    "        \n",
    "        iteration = (epoch - 1) * n_train_batches + i\n",
    "\n",
    "        if (iteration + 1) % valid_freq == 0:\n",
    "            validation_ll = np.sum(model.validate_model())\n",
    "            #################################\n",
    "            # track and save training stats #\n",
    "            #################################\n",
    "            training_step = {\n",
    "                'epoch': epoch, \n",
    "                'minibatch': i + 1, \n",
    "                'batches': n_train_batches, \n",
    "                'train_ll': training_ll * n_train_batches, \n",
    "                'valid_ll': validation_ll, \n",
    "                'valid_err': None,\n",
    "                'train_err': None,\n",
    "            }\n",
    "            training_frame.loc[step] = training_step\n",
    "            #################################\n",
    "\n",
    "            # check prediction accuracy\n",
    "            error = np.mean(model.predict_model())\n",
    "            training_frame.loc[step, 'valid_err'] = error\n",
    "\n",
    "            \n",
    "            \n",
    "            if validation_ll < best_validation_ll:\n",
    "                print(('epoch {:d}, minibatch {:d}/{:d}, '\n",
    "                       'validation likelihood {:.2f}').format(\n",
    "                        epoch, i + 1, n_train_batches, validation_ll))\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if validation_ll < best_validation_ll * config['improvement_threshold']:\n",
    "                    config['patience'] = max(config['patience'], iteration * config['patience_increase'])\n",
    "\n",
    "                # keep track of best validation score\n",
    "                best_validation_ll = validation_ll\n",
    "\n",
    "                error = np.mean(model.predict_model())\n",
    "                training_frame.loc[step, 'valid_err'] = error\n",
    "                print('validation error  {:.2%}'.format(error))\n",
    "\n",
    "                # save the best model\n",
    "                with open(filename, 'wb') as f:\n",
    "                    pickle.dump([model, config], f)\n",
    "\n",
    "            step = step + 1\n",
    "\n",
    "        if epoch > 200:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "#############################\n",
    "# parameter extraction step #\n",
    "#############################\n",
    "print('processing model statistics...')\n",
    "with open(filename, 'rb') as f:\n",
    "    model, config = pickle.load(f)\n",
    "\n",
    "# format model beta params and ASCs\n",
    "model_stat = {}\n",
    "if config['model_type'] in ['MNL', 'ResNet']:\n",
    "    beta = model.beta.eval().round(3)\n",
    "    beta_df = pd.DataFrame(beta, index=config['variables'], columns=config['choices'])\n",
    "    model_stat['beta_params'] = beta_df\n",
    "\n",
    "    asc = model.params[1].eval().round(3)\n",
    "    asc_df = pd.DataFrame(asc.reshape((1,-1)), index=['ASC'], columns=config['choices'])\n",
    "    model_stat['asc_params'] = asc_df\n",
    "\n",
    "    # compute std. err and t-stat\n",
    "    h = np.mean([model.hessians(i) for i in range(n_train_batches)], axis=0)\n",
    "    model_stat['beta_stderr'] = pd.DataFrame(\n",
    "        np.sqrt(1/np.diag(h[0]).reshape((config['n_vars'], config['n_choices'])))/(batch_size-1), \n",
    "        index=config['variables'], \n",
    "        columns=config['choices']\n",
    "    )\n",
    "    model_stat['beta_t_stat'] = pd.DataFrame(\n",
    "        beta / model_stat['beta_stderr'], index=config['variables'], columns=config['choices'])\n",
    "    \n",
    "# format ResNet residual matrix\n",
    "if config['model_type'] == 'ResNet':\n",
    "    model_stat['residual_matrix'] = []\n",
    "    for l in range(config['n_layers']):\n",
    "        # create a pandas correlation matrix table\n",
    "        mat = model.resnet_layers[l].params[0].eval().round(2)\n",
    "        df = pd.DataFrame(\n",
    "            data=mat, index=config['choices'], columns=config['choices'])\n",
    "        model_stat['residual_matrix'].append(df)\n",
    "\n",
    "# misc: runtime and training curves\n",
    "model_stat['run_time'] = str(np.round(run_time / 60., 3))+' minutes'\n",
    "model_stat['training_frame'] = training_frame\n",
    "\n",
    "# re-save model, configuration and statistics     \n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump([model, config, model_stat], f)        \n",
    "#############################\n",
    "\n",
    "# print final verbose output\n",
    "print(('Optimization complete with best validation likelihood of {:.2f}, '\n",
    "       'and validation error of {:.2%}').format(best_validation_ll, error))\n",
    "print(('The code run for {:d} epochs, with {:.2f} epochs/sec').format(\n",
    "        epoch, 1. * epoch / run_time))\n",
    "print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['model_type'] == 'ResNet': \n",
    "#     print(model_stat['residual_matrix'])\n",
    "# MNL Optimization complete with best validation likelihood of 15873.71, and validation error of 27.93% $$\n",
    "# MLP-2 Optimization complete with best validation likelihood of 15643.12, and validation error of 27.56% $$\n",
    "# MLP-4 Optimization complete with best validation likelihood of 17406.30, and validation error of 29.74% $$\n",
    "# MLP-8 Optimization complete with best validation likelihood of 17370.73, and validation error of 29.70% $$\n",
    "# MLP-16 Optimization complete with best validation likelihood of 17403.81, and validation error of 29.69% $$\n",
    "\n",
    "# ResNet-2 Optimization complete with best validation likelihood of 13443.44, and validation error of 23.87% $$\n",
    "# ResNet-4 Optimization complete with best validation likelihood of 13082.63, and validation error of 23.58% $$\n",
    "# ResNet-8 Optimization complete with best validation likelihood of 12894.89, and validation error of 23.33% $$\n",
    "# ResNet-16 Optimization complete with best validation likelihood of 12938.32, and validation error of 23.30% $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'ResNet16_bestmodel.pkl'\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "model, config, model_stat = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat['beta_params'].stack().reset_index().iloc[[0,1,2,10,11,15,18,28,36,70,72,73,77,79,80,84,87,91,94,133,134,135,140,142,144,148]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
